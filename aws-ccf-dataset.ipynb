{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "Intro TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook requires the following additional libraries\n",
    "# (please install using the preferred method for your environment, e.g. pip, conda):\n",
    "#\n",
    "# lib1 >= ver1\n",
    "# lib2 >= ver2\n",
    "# lib3 >= ver3 \n",
    "\n",
    "# Import the libraries required for this notebook\n",
    "# Built-ins\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import os.path\n",
    "import platform\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Installed libraries\n",
    "%pip install requests duckdb warcio cdx_toolkit pyarrow pandas polars cdxj-indexer setuptools\n",
    "import duckdb\n",
    "import requests\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "# TODO: Potentially switch to using AWS python libraries\n",
    "#import boto3, polars, matplotlib.pyplot as plt\n",
    "#from botocore import UNSIGNED\n",
    "#from botocore.config import Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba9f12",
   "metadata": {},
   "source": [
    "### Task 1: Look at the crawl data\n",
    "\n",
    "Downloading explanations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary downloading from github\n",
    "!curl -O https://raw.githubusercontent.com/commoncrawl/whirlwind-python/main/whirlwind.warc.gz\n",
    "!curl -O https://raw.githubusercontent.com/commoncrawl/whirlwind-python/main/whirlwind.warc.wat.gz\n",
    "!curl -O https://raw.githubusercontent.com/commoncrawl/whirlwind-python/main/whirlwind.warc.wet.gz\n",
    "\n",
    "WEB_ARCHIVE_FILES = ['whirlwind.warc.gz',\n",
    "                     'whirlwind.warc.wat.gz',\n",
    "                     'whirlwind.warc.wet.gz']\n",
    "\n",
    "\n",
    "#TODO: Potentially switch to loading data from AWS S3 directly using boto3\n",
    "\n",
    "# Location of the S3 bucket for this dataset\n",
    "# bucket = \"amazon-last-mile-challenges\"\n",
    "\n",
    "# List the top level of the bucket using boto3. Because this is a public bucket, we don't need to sign requests.\n",
    "# Here we set the signature version to unsigned, which is required for public buckets.3\n",
    "# s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# Print the items in the top-level prefixes\n",
    "# for item in s3.list_objects_v2(Bucket=bucket, Delimiter='/')['CommonPrefixes']:\n",
    "#    print(item['Prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc64ba4",
   "metadata": {},
   "source": [
    "Data format explanations here.\n",
    "\n",
    "1. WARC\n",
    "2. WET\n",
    "3. WAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e0f51",
   "metadata": {},
   "source": [
    "### Task 2: Iterate over WARC, WET, and WAT files\n",
    "\n",
    "warcio explanations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warcio_iterator(file):\n",
    "    # Iterates over a given web archive file\n",
    "    with open(file, 'rb') as stream:\n",
    "            for record in ArchiveIterator(stream):\n",
    "                print('File:', file)\n",
    "                print(' ', 'WARC-Type:', record.rec_type)\n",
    "                if record.rec_type in {'request', 'response', 'conversion', 'metadata'}:\n",
    "                    print('   ', 'WARC-Target-URI', record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "for file in WEB_ARCHIVE_FILES:\n",
    "    warcio_iterator(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bcf-ec40-432f-a31f-4477efa205ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "The ArchiveIterator reads the WARC content in a single pass and allows us to access the attributes of each record (e.g. the record type through record.rec_type).\n",
    "\n",
    "The output has three sections, one each for the WARC, WET, and WAT. For each one, it prints the record types we saw before, plus the WARC-Target-URI for those record types that have it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aade68",
   "metadata": {},
   "source": [
    "### Task 3: Index the WARC, WET, and WAT\n",
    "\n",
    "Explain CDX index vs columnar index.\n",
    "We will start with CDX index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362bd15",
   "metadata": {},
   "source": [
    "##### Task 3.a: CDX(J) index\n",
    "\n",
    "CDX(J) explanation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create *.cdxj index files from the local warcs\n",
    "!cdxj-indexer whirlwind.warc.gz > whirlwind.warc.cdxj\n",
    "!cdxj-indexer --records conversion whirlwind.warc.wet.gz > whirlwind.warc.wet.cdxj\n",
    "!cdxj-indexer whirlwind.warc.wat.gz > whirlwind.warc.wat.cdxj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686af22a",
   "metadata": {},
   "source": [
    "Now look at the `.cdxj` files with cat `whirlwind*.cdxj`. You'll see that each file has one entry in the index. The WARC only has the response record indexed, since by default cdxj-indexer guesses that you won't ever want to random-access the request or metadata. WET and WAT have the conversion and metadata records indexed (Common Crawl doesn't publish a WET or WAT index, just WARC).\n",
    "\n",
    "For each of these records, there's one text line in the index - yes, it's a flat file! It starts with a string like `org,wikipedia,an)/wiki/escopete 20240518015810`, followed by a JSON blob. The starting string is the primary key of the index. The first thing is a SURT (Sort-friendly URI Reordering Transform). The big integer is a date, in ISO-8601 format with the delimiters removed.\n",
    "\n",
    "What is the purpose of this funky format? It's done this way because these flat files (300 gigabytes total per crawl) can be sorted on the primary key using any out-of-core sort utility e.g. the standard Linux sort, or one of the Hadoop-based out-of-core sort functions.\n",
    "\n",
    "The JSON blob has enough information to extract individual records: it says which warc file the record is in, and the offset and length of the record. We'll use that in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25fd6e",
   "metadata": {},
   "source": [
    "### Task 4: Use the CDXJ index to extract raw content from the local WARC, WET, and WAT\n",
    "\n",
    "Normally, compressed files aren't random access. However, the WARC files use a trick to make this possible, which is that every record needs to be separately compressed. The `gzip` compression utility supports this, but it's rarely used.\n",
    "\n",
    "To extract one record from a warc file, all you need to know is the filename and the offset into the file. If you're reading over the web, then it really helps to know the exact length of the record.\n",
    "\n",
    "Let's get a set of extractions from your local whirlwind.*.gz files with warcio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23567bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction.* from local warcs, the offset numbers are from the cdxj index\n",
    "\n",
    "!warcio extract --payload whirlwind.warc.gz 1023 > extraction.html\n",
    "!warcio extract --payload whirlwind.warc.wet.gz 466 > extraction.txt\n",
    "!warcio extract --payload whirlwind.warc.wat.gz 443 > extraction.json\n",
    "\n",
    "# Hint: You can try python -m json.tool extraction.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ba971",
   "metadata": {},
   "source": [
    "The offset numbers in the Makefile are the same ones as in the index. Look at the three output files:   `extraction.html`, `extraction.txt`, and `extraction.json` (pretty-print the json with `python -m json.tool extraction.json`).\n",
    "\n",
    "Notice that we extracted HTML from the WARC, text from WET, and JSON from the WAT (as shown in the different file extensions). This is because the payload in each file type is formatted differently!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9062e1",
   "metadata": {},
   "source": [
    "### Task 5: Wreck the WARC by compressing it wrong\n",
    "\n",
    "As mentioned earlier, WARC/WET/WAT files look like they're gzipped, but they're actually gzipped in a particular way that allows random access. This means that you can't gunzip and then gzip a warc without wrecking random access. This example:\n",
    "\n",
    "- creates a copy of one of the warc files in the repo\n",
    "- uncompresses it\n",
    "- recompresses it the wrong way\n",
    "- runs warcio-iterator over it to show that it triggers an error\n",
    "- recompresses it the right way using warcio recompress\n",
    "- shows that this compressed file works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7209334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will break and then fix this warc\n",
    "!cp whirlwind.warc.gz testing.warc.gz\n",
    "!rm -f testing.warc\n",
    "!gzip -d testing.warc.gz\n",
    "\t\n",
    "# Iterate over this uncompressed warc: works\n",
    "warcio_iterator('testing.warc')\n",
    "\n",
    "# Compress it the wrong way\n",
    "!gzip testing.warc\n",
    "\n",
    "# Iterating over this compressed warc fails\n",
    "try:\n",
    "    print('\\nThis wont work!')\n",
    "    warcio_iterator('testing.warc.gz')\n",
    "except Exception as e:\n",
    "    print(f\"Error iterating over malformed warc.gz: {e}\")\n",
    "\n",
    "# Now let's do it the right way\n",
    "!gzip -d testing.warc.gz\n",
    "!warcio recompress testing.warc testing.warc.gz\n",
    "\n",
    "# And now iterating works\n",
    "print('\\nThis should work:')\n",
    "warcio_iterator('testing.warc.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717987e-c9c9-4c0f-9f2e-a8d761492625",
   "metadata": {},
   "source": [
    "Make sure you compress WARCs the right way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6f391",
   "metadata": {},
   "source": [
    "### Task 6: Use cdx_toolkit to query the full CDX index and download those captures from AWS S3\n",
    "\n",
    "Some of our users only want to download a small subset of the crawl. They want to run queries against an index, either the CDX index we just talked about, or in the columnar index, which we'll talk about later.\n",
    "\n",
    "The cdx_toolkit is a set of tools for working with CDX indices of web crawls and archives. It knows how to query the CDX index across all of our crawls and also can create WARCs of just the records you want. We will fetch the same record from Wikipedia that we've been using for the whirlwind tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up this capture in the comoncrawl cdx index\n",
    "!cdxt --limit 1 --crawl CC-MAIN-2024-22 --from 20240518015810 --to 20240518015810 iter an.wikipedia.org/wiki/Escopete\n",
    "\n",
    "#Extract the content from the commoncrawl s3 bucket\n",
    "!rm -f TEST-000000.extracted.warc.gz\n",
    "!cdxt --limit 1 --crawl CC-MAIN-2024-22 --from 20240518015810 --to 20240518015810 warc an.wikipedia.org/wiki/Escopete\n",
    "\n",
    "# Index this new warc\n",
    "!cdxj-indexer TEST-000000.extracted.warc.gz  > TEST-000000.extracted.warc.cdxj\n",
    "!cat TEST-000000.extracted.warc.cdxj\n",
    "\n",
    "# Iterate this new warc\n",
    "warcio_iterator('TEST-000000.extracted.warc.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf3b6c",
   "metadata": {},
   "source": [
    "We look up the capture using the cdxt commands by specifying the exact URL (an.wikipedia.org/wiki/Escopete) and the date of its capture (20240518015810). The output is the WARC file TEST-000000.extracted.warc.gz which contains a warcinfo record explaining what the WARC is, followed by the response record we requested. The Makefile target then runs cdxj-indexer on this new WARC to make a CDXJ index of it as in Task 3, and finally iterates over the WARC using warcio-iterator.py as in Task 2.\n",
    "\n",
    "If you dig into cdx_toolkit's code, you'll find that it is using the offset and length of the WARC record, as returned by the CDX index query, to make a HTTP byte range request to S3 to download the single WARC record we want. It only downloads the response WARC record because our CDX index only has the response records indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10164f",
   "metadata": {},
   "source": [
    "### Task 7: Find the right part of the columnar index\n",
    "\n",
    "Now let's look at the columnar index, the other kind of index that Common Crawl makes available. This index is stored in parquet files so you can access it using SQL-based tools like AWS Athena and duckdb as well as through tables in your favorite table packages such as pandas, pyarrow, and polars.\n",
    "\n",
    "We could read the data directly from our index in our S3 bucket and analyse it in the cloud through AWS Athena. However, this is a managed service that costs money to use (though usually a small amount). You can read about using it here. This whirlwind tour will only use the free method of either fetching data from outside of AWS (which is kind of slow), or making a local copy of a single columnar index (300 gigabytes per monthly crawl), and then using that.\n",
    "\n",
    "The columnar index is divided up into a separate index per crawl, which Athena or duckdb can stitch together. The cdx index is similarly divided up, but cdx_toolkit hides that detail from you.\n",
    "\n",
    "For the purposes of this whirlwind tour, we don't want to configure all the crawl indices because it would be slow. So let's start by figuring out which crawl was ongoing on the date `20240518015810`, and then we'll work with just that one crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5cdf3",
   "metadata": {},
   "source": [
    "##### Task 7.a: Downloading collinfo.json\n",
    "\n",
    "We're going to use the collinfo.json file to find out which crawl we want. This file includes the dates for the start and end of every crawl and is available through the Common Crawl website at index.commoncrawl.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef33976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download collinfo.json so we can find out the crawl name\n",
    "!curl -O https://index.commoncrawl.org/collinfo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a1a1a",
   "metadata": {},
   "source": [
    "The date of our test record is `20240518015810`, which is `2024-05-18T01:58:10` if you add the delimiters back in. We can scroll through the records in `collinfo.json` and look at the from/to values to find the right crawl: `CC-MAIN-2024-22`. Now we know the crawl name, we can access the correct fraction of the index without having to read the metadata of all the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd9347",
   "metadata": {},
   "source": [
    "### Task 8: Query using the columnar index + DuckDB from outside AWS\n",
    "\n",
    "A single crawl columnar index is around 300 gigabytes. If you don't have a lot of disk space, but you do have a lot of time, you can directly access the index stored on AWS S3. We're going to do just that, and then use DuckDB to make an SQL query against the index to find our webpage. We'll be running the following query:\n",
    "\n",
    "```\n",
    "    SELECT\n",
    "      *\n",
    "    FROM ccindex\n",
    "    WHERE subset = 'warc'\n",
    "      AND crawl = 'CC-MAIN-2024-22'\n",
    "      AND url_host_tld = 'org' -- help the query optimizer\n",
    "      AND url_host_registered_domain = 'wikipedia.org' -- ditto\n",
    "      AND url = 'https://an.wikipedia.org/wiki/Escopete'\n",
    "    ;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079804c-1fbc-4e54-9368-e5d7644e72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_download_advice(prefix, crawl):\n",
    "    print('Do you need to download this index?')\n",
    "    print(f' mkdir -p {prefix}/commmoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/')\n",
    "    print(f' cd {prefix}/commmoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/')\n",
    "    print(f' aws s3 sync s3://commoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/ .')\n",
    "\n",
    "\n",
    "def print_row_as_cdxj(row):\n",
    "    df = row.fetchdf()\n",
    "    for ro in df.itertuples(index=False):\n",
    "        d = ro._asdict()\n",
    "        cdxjd = {\n",
    "            'url': d['url'],\n",
    "            'mime': d['content_mime_type'],\n",
    "            'status': str(d['fetch_status']),\n",
    "            'digest': 'sha1:' + d['content_digest'],\n",
    "            'length': str(d['warc_record_length']),\n",
    "            'offset': str(d['warc_record_offset']),\n",
    "            'filename': d['warc_filename'],\n",
    "        }\n",
    "\n",
    "        timestamp = d['fetch_time'].isoformat(sep='T')\n",
    "        timestamp = timestamp.translate(str.maketrans('', '', '-T :Z')).replace('+0000', '')\n",
    "\n",
    "        print(d['url_surtkey'], timestamp, json.dumps(cdxjd))\n",
    "\n",
    "\n",
    "def print_row_as_kv_list(row):\n",
    "    df = row.fetchdf()\n",
    "    for ro in df.itertuples(index=False):\n",
    "        d = ro._asdict()\n",
    "        for k, v in d.items():\n",
    "            print(' ', k, v)\n",
    "\n",
    "\n",
    "def get_files(algo, crawl):\n",
    "    if algo == 's3_glob':\n",
    "        # 403 errors with and without credentials. you have to be commoncrawl-pds\n",
    "        files = f's3://commoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/*.parquet'\n",
    "        raise NotImplementedError('will cause a 403')\n",
    "    elif algo == 'local_files':\n",
    "        files = os.path.expanduser(f'~/commmoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/*.parquet')\n",
    "        files = glob.glob(files)\n",
    "        # did we already download? we expect 300 files of about a gigabyte\n",
    "        if len(files) < 250:\n",
    "            index_download_advice('~', crawl)\n",
    "            exit(1)\n",
    "    elif algo == 'ccf_local_files':\n",
    "        files = glob.glob(f'/home/cc-pds/commoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/*.parquet')\n",
    "        if len(files) < 250:\n",
    "            index_download_advice('/home/cc-pds', crawl)\n",
    "            exit(1)\n",
    "    elif algo == 'cloudfront_glob':\n",
    "        # duckdb can't glob this, same reason as s3_glob above\n",
    "        files = f'https://data.commoncrawl.org/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/*.parquet'\n",
    "        raise NotImplementedError('duckdb will throw an error because it cannot glob this')\n",
    "    elif algo == 'cloudfront':\n",
    "        prefix = f's3://commoncrawl/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/'\n",
    "        external_prefix = f'https://data.commoncrawl.org/cc-index/table/cc-main/warc/crawl={crawl}/subset=warc/'\n",
    "        file_file = f'{crawl}.warc.paths.gz'\n",
    "\n",
    "        with gzip.open(file_file, mode='rt', encoding='utf8') as fd:\n",
    "            files = fd.read().splitlines()\n",
    "            files = [external_prefix+f for f in files]\n",
    "    else:\n",
    "        raise NotImplementedError('algo: '+algo)\n",
    "    return files\n",
    "\n",
    "\n",
    "def run_duckdb_query(algo, crawl):\n",
    "    windows = True if platform.system() == 'Windows' else False\n",
    "    if windows:\n",
    "        # windows stdout is often cp1252\n",
    "        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "    files = get_files(algo, crawl)\n",
    "    retries_left = 100\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            ccindex = duckdb.read_parquet(files, hive_partitioning=True)\n",
    "            break\n",
    "        except (duckdb.HTTPException, duckdb.InvalidInputException) as e:\n",
    "            # read_parquet exception seen: HTTPException(\"HTTP Error: HTTP GET error on 'https://...' (HTTP 403)\")\n",
    "            # duckdb.duckdb.InvalidInputException: Invalid Input Error: No magic bytes found at end of file 'https://...'\n",
    "            print('read_parquet exception seen:', repr(e), file=sys.stderr)\n",
    "            if retries_left:\n",
    "                print('sleeping for 60s', file=sys.stderr)\n",
    "                time.sleep(60)\n",
    "                retries_left -= 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    duckdb.sql('SET enable_progress_bar = true;')\n",
    "    duckdb.sql('SET http_retries = 100;')\n",
    "    #duckdb.sql(\"SET enable_http_logging = true;SET http_logging_output = 'duck.http.log'\")\n",
    "\n",
    "    print('total records for crawl:', crawl)\n",
    "    retries_left = 100\n",
    "    while True:\n",
    "        try:\n",
    "            print(duckdb.sql('SELECT COUNT(*) FROM ccindex;'))\n",
    "            break\n",
    "        except duckdb.InvalidInputException as e:\n",
    "            # duckdb.duckdb.InvalidInputException: Invalid Input Error: No magic bytes found at end of file 'https://...'\n",
    "            print('duckdb exception seen:', repr(e), file=sys.stderr)\n",
    "            if retries_left:\n",
    "                print('sleeping for 10s', file=sys.stderr)\n",
    "                time.sleep(10)\n",
    "                retries_left -= 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    sq2 = f'''\n",
    "    select\n",
    "      *\n",
    "    from ccindex\n",
    "    where subset = 'warc'\n",
    "      and crawl = 'CC-MAIN-2024-22'\n",
    "      and url_host_tld = 'org' -- help the query optimizer\n",
    "      and url_host_registered_domain = 'wikipedia.org' -- ditto\n",
    "      and url = 'https://an.wikipedia.org/wiki/Escopete'\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "    row2 = duckdb.sql(sq2)\n",
    "    print('our one row')\n",
    "    while True:\n",
    "        try:\n",
    "            row2.show()\n",
    "            break\n",
    "        except duckdb.InvalidInputException as e:\n",
    "            # duckdb.duckdb.InvalidInputException: Invalid Input Error: No magic bytes found at end of file 'https://...'\n",
    "            print('duckdb exception seen:', repr(e), file=sys.stderr)\n",
    "            if retries_left:\n",
    "                print('sleeping for 10s', file=sys.stderr)\n",
    "                time.sleep(10)\n",
    "                retries_left -= 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    print('writing our one row to a local parquet file, whirlwind.parquet')\n",
    "    row2.write_parquet('whirlwind.parquet')\n",
    "\n",
    "    cclocal = duckdb.read_parquet('whirlwind.parquet')\n",
    "\n",
    "    print('total records for local whirlwind.parquet should be 1')\n",
    "    print(duckdb.sql('SELECT COUNT(*) FROM cclocal;'))\n",
    "\n",
    "    sq3 = sq2.replace('ccindex', 'cclocal')\n",
    "    row3 = duckdb.sql(sq3)\n",
    "    print('our one row, locally')\n",
    "    row3.show()\n",
    "\n",
    "    print('complete row:')\n",
    "    print_row_as_kv_list(row3)\n",
    "    print('')\n",
    "\n",
    "    print('equivalent to cdxj:')\n",
    "    print_row_as_cdxj(row3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606c455-a2f4-4c3a-9d81-fd9575b9b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning! this might take 1-10 minutes\"\n",
    "\n",
    "# Temporary downloading from github\n",
    "!curl -O https://raw.githubusercontent.com/commoncrawl/whirlwind-python/main/CC-MAIN-2024-22.warc.paths.gz\n",
    "\n",
    "crawl = 'CC-MAIN-2024-22'\n",
    "run_duckdb_query('cloudfront', crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8416417",
   "metadata": {},
   "source": [
    "On a machine with a 1 gigabit network connection and many cores, this should take about one minute total, and uses 8 cores.\n",
    "\n",
    "The above code accesses the relevant part of the index for our crawl (`CC-MAIN-2024-22`) and then counts the number of records in that crawl (2709877975!). The code runs the SQL query we saw before which should match the single response record we want.\n",
    "\n",
    "The program then writes that one record into a local Parquet file, does a second query that returns that one record, and shows the full contents of the record. We can see that the complete row contains many columns containing different information associated with our record. Finally, it converts the row to the CDXJ format we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303cbc82",
   "metadata": {},
   "source": [
    "### Bonus Task! Download a full crawl index and query with DuckDB\n",
    "\n",
    "If you want to run many of these queries, and you have a lot of disk space, you'll want to download the 300 gigabyte index and query it repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df75c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning! 300 gigabyte download\n",
    "\n",
    "# TODO: Skipping this for now, too large a download potentially for SageMaker\n",
    "# run_duckdb_query('local_files', crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9934e",
   "metadata": {},
   "source": [
    "(**Bonus bonus:** If you happen to be using the Common Crawl Foundation development server, we've already downloaded these files, so you can also try `run_duckdb_query('ccf_local_files', crawl)`.)\n",
    "\n",
    "All of these scripts run the same SQL query and should return the same record (written as a parquet file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2f5a5",
   "metadata": {},
   "source": [
    "### Bonus Task 2! Combine some steps\n",
    "\n",
    "Use the DuckDb techniques from Task 8 and the Index Server to find a new webpage in the archives.\n",
    "Note its url, warc, and timestamp.\n",
    "Now open up the Makefile from Task 6 and look at the actions from the `cdx_toolkit` section.\n",
    "Repeat the `cdx_toolkit` steps, but for the page and date range you found above.\n",
    "\n",
    "### Congratulations!\n",
    "You have completed the Whirlwind Tour of Common Crawl's Datasets using Python! You should now understand different filetypes we have in our corpus and how to interact with Common Crawl's datasets using Python. To see what other people have done with our data, see the Examples page on our website. Why not join our Discord through the Community tab?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
